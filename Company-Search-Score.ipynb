{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Search Google News\n",
    "def search_google_news(query, num_results=10):\n",
    "    print(f\"Searching Google News for: {query}\")\n",
    "    search_query = f'{query} site:news.google.com'\n",
    "    results = search(search_query, num_results=num_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scrape News Articles\n",
    "def scrape_article(url, timeout=10):\n",
    "    print(f\"Scraping article: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # grab the title of the article\n",
    "        title = soup.find('h1').get_text()\n",
    "        # return the title\n",
    "        article = title\n",
    "        print(f\"Scraped article: {article[:50]}...\")\n",
    "        return article\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout while scraping {url}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocess and Predict Sentiment\n",
    "def preprocess_articles(articles):\n",
    "    print(\"Preprocessing articles...\")\n",
    "    encodings = tokenizer(articles, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "    return encodings\n",
    "\n",
    "def predict_sentiment(encodings):\n",
    "    print(\"Predicting sentiment...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "    return predictions\n",
    "\n",
    "def decode_sentiment(predictions):\n",
    "    print(\"Decoding sentiment predictions...\")\n",
    "    sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "    return [sentiment_map[pred.item()] for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_symbol = 'AAPL'  # Example stock symbol\n",
    "news_urls = search_google_news(f'{stock_symbol} stock')\n",
    "\n",
    "# Scrape articles with timeout using concurrent.futures\n",
    "articles = []\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    future_to_url = {executor.submit(scrape_article, url): url for url in news_urls}\n",
    "    for future in as_completed(future_to_url, timeout=20):  # Set the overall timeout for all tasks\n",
    "        url = future_to_url[future]\n",
    "        try:\n",
    "            article = future.result(timeout=20)  # Set timeout for each individual task\n",
    "            articles.append(article)\n",
    "        except TimeoutError:\n",
    "            print(f\"Timeout while scraping {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "state_dict = torch.load('path_to_your_model_state_dict.pth', map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "# Preprocess articles\n",
    "news_encodings = preprocess_articles([article for article in articles if article])\n",
    "\n",
    "# Predict sentiment\n",
    "predictions = predict_sentiment(news_encodings)\n",
    "\n",
    "# Decode and calculate average sentiment score\n",
    "sentiment_labels = decode_sentiment(predictions)\n",
    "# Convert sentiment labels to scores\n",
    "sentiment_scores = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "scores = [sentiment_scores[label] for label in sentiment_labels]\n",
    "average_sentiment = sum(scores) / len(scores) if scores else 0  # Handle case with no valid scores\n",
    "print(f'Overall Sentiment Score: {average_sentiment}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
